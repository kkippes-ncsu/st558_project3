---
title: "Modeling"
author: Kayla Kippes
format: html
editor: visual
output-dir: docs
---

## Introduction

The Diabetes Health Indicators Data set has 253,680 survey responses to the CDC's BRFSS2015 (Behavioral Risk Factor Surveillance System 2015). The primary response variable is "Diabetes_binary" which is in two classes: 0 for no diabetes and 1 for prediabetes/diabetes. This data is unbalanced with the largest class being the no diabetes class. Additionally, this data set has 21 variables.

In this analysis, we will be using log loss as our metric to evaluate the best model and will use 5 fold cross-validation to select the best model for all model types. The goal is to pick the best model that gives the best prediction whether or not someone has diabetes.

First we will split the data into training and test sets to use for modeling. By doing this, it will help with over fitting as our goal is for the model to predict well on unseen data. We will use logistic regression, classification trees, and random forest models.

## Data

```{r warning=FALSE, message=FALSE}
## load the necessary packages
library(tidyverse)
library(tidymodels)
library(caret)
library(yardstick)

## read in the data
diabetes_data <- read.csv("diabetes_binary_health_indicators_BRFSS2015.csv")

## factor all necessary variables used for modeling and select ones wanted
diabetes_modeling <- diabetes_data |>
  mutate(Diabetes = factor(Diabetes_binary,
                           levels=c("0","1"),
                           labels=c("No_Diabetes","Prediabetes_or_Diabetes")),
         HighBP = factor(HighBP,
                         levels=c("0","1"),
                         labels=c("No_High BP","High_BP")),
         HighChol = factor(HighChol,
                           levels=c("0","1"),
                           labels=c("No_High_Cholesterol","High_Cholesterol")),
         Smoker = factor(Smoker,
                         levels=c("1","0"),
                         labels=c("Smoker","Not_Smoker")),
         Stroke = factor(Stroke,
                         levels=c("1","0"),
                         labels=c("Had_Stroke","No_Stroke")),
         Heart_Disease_or_Attack = factor(HeartDiseaseorAttack,
                                          levels=c("1","0"),
                                          labels=c("Heart_Disease","No_Heart_Disease")),
         Heavy_Alcohol_Consumption = factor(HvyAlcoholConsump,
                                            levels=c("1","0"),
                                            labels=c("Heavy_Alcohol",
                                                     "No_Heavy_Alcohol")),
         General_Health = factor(GenHlth,
                                 levels=c("1","2","3","4","5"),
                                 labels=c("Excellent","Very_Good","Good","Fair","Poor")),
         Sex = factor(Sex,
                      levels=c("0","1"),
                      labels=c("Female","Male")),
         Age = factor(Age,
                      levels=c("1","2","3","4","5","6","7",
                               "8","9","10","11","12","13"),
                      labels=c("18-24","25-29","30-34","35-39","40-44",
                               "45-49","50-54","55-59","60-64","65-69",
                               "70-74","75-79","80+")),
         Income = factor(Income,
                         levels=c("1","2","3","4","5","6","7","8"),
                         labels=c("<$10,000",
                                  "$10,000-$15,000",
                                  "$15,000-$20,000",
                                  "$20,000-$25,000",
                                  "$25,000-$35,000",
                                  "$35,000-$50,000",
                                  "$50,000-$75,000",
                                  "$75,000+")),
         ) |>
  select(Diabetes, HighBP, HighChol, BMI, Smoker, Stroke, Heart_Disease_or_Attack,
         Heavy_Alcohol_Consumption, General_Health, Sex, Age, Income)
```

Now that we have selected the data for the models, we are going to split it into the train and test sets.

```{r warning=FALSE, message=FALSE}
## set seed
set.seed(101)

## split the data into train and test sets
diabetes_split <- initial_split(diabetes_modeling, prop = 0.7)
train <- training(diabetes_split)
test <- testing(diabetes_split)
```

We also need to establish our 5 fold cross-validation.

```{r warning=FALSE, message=FALSE}
tr_ctrl <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = mnLogLoss
)
```

Now let's create our models.

### Logistic Regression

Logistic regression is a generalized linear modeling technique used to predict the probability of a binary outcome and it cane be used for when the outcome is categorical. It uses a logit function to model the relationship between predictors and the log-odds of an event occurring. We are using it for this data because our response variable, diabetes, is a binary classification which is what these types of models are designed for. Below we will look at 3 different LR models and choose our best one.

```{r warning=FALSE, message=FALSE}
## LR model 1
LR1 <- train(
  Diabetes ~ .,
  data = train,
  method = "glm",
  family = "binomial",
  trControl = tr_ctrl,
  metric = "logLoss"
)

## LR model 2
LR2 <- train(
  Diabetes ~ HighChol + HighBP + Smoker + Stroke,
  data = train,
  method = "glm",
  family = "binomial",
  trControl = tr_ctrl,
  metric = "logLoss"
)

## LR model 2
LR3 <- train(
  Diabetes ~ General_Health + Sex + Age + Income,
  data = train,
  method = "glm",
  family = "binomial",
  trControl = tr_ctrl,
  metric = "logLoss"
)

## compare the 3 models
LogLoss_Results <- rbind(
  data.frame(Model = "Model 1", LR1$results),
  data.frame(Model = "Model 2", LR2$results),
  data.frame(Model = "Model 3", LR3$results)
)

LogLoss_Results
```

Based off of our results, we can conclude that the model with the lowest log loss is our best model so in this case it will be Model 1 which used all of the preselected predictors.

### Classification Trees

Tree based methods attempt to split up predictor space into regions. Within each region, a different prediction can be made. Classification trees are used if the goal is to classify (predict) group membership. Usually the most prevalent class in the region is used as the prediction. In this case, predicting diabetes from the various explanatory variables may involve complex interactions so using a classification tree will help capture those. Decision trees help filter for the important model features and  are easy to interpret.

```{r warning=FALSE, message=FALSE}
## create recipe
tree_rec <- recipe(Diabetes ~ ., data = train) |>
  step_dummy(all_nominal_predictors()) |>
  step_normalize(all_numeric(), -all_outcomes())

## define the tuning grid
tree_grid <- expand.grid(cp = seq(0.001, 0.05, by = 0.005))

## fit the model
tree_model <- train(
  tree_rec,
  data = train,
  method = "rpart",
  trControl = tr_ctrl,
  tuneGrid = tree_grid,
  metric = "logLoss"
)

# Check best
tree_model$results |> arrange(logLoss)
```
Our best model had a complexity parameter of 0.001 with a log loss of 0.3576.

### Random Forest

Random Forest is an ensemble method that builds multiple decision trees during training and combines their outputs to improve predictive performance. Each tree is trained on a random subset of the training data (bagging) as well as a random subset of features which helps reduce over fitting. Multiple tress are created from the bootstrap samples to increase stability.

For classification, the final prediction is performed via a majority vote of all the ensemble trees. Random forests tend to give better accuracy than a single classification tree.

```{r warning=FALSE, message=FALSE}
## define recipe
RF_rec <- recipe(Diabetes ~ BMI + Age + HighChol + General_Health, data = train) |>
  step_normalize(BMI) |>
  step_dummy(Age, HighChol, General_Health)

## set model spec
rf_spec <- rand_forest(mtry = tune(), trees = 100) |>
 set_engine("ranger") |>
 set_mode("classification")

## create workflow
rf_wkf <- workflow() |>
  add_recipe(RF_rec) |>
  add_model(rf_spec)

## fit the model
rf_fit <- rf_wkf |>
  tune_grid(resamples = vfold_cv(train, 5),
            grid = 7,
            metrics = metric_set(mn_log_loss))

## get best model
rf_fit |> 
  collect_metrics() |>
  filter(.metric == "mn_log_loss")
rf_best_params <- select_best(rf_fit, metric = "mn_log_loss")
rf_best_params

## finalize workflow
rf_final_wkf <- rf_wkf |>
 finalize_workflow(rf_best_params)
rf_final_fit <- rf_final_wkf |>
 last_fit(diabetes_split, metrics = metric_set(mn_log_loss))

## final fit
rf_final_fit |> collect_metrics()
```

## Final Model Selection

Based on the three best models from above we found that out of those 3, random forest gave us the best model on the test data.